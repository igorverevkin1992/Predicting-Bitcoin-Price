{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a9eb42e",
   "metadata": {},
   "source": [
    "# CAPSTONE 3. Predicting Major Cryptocurrencies Prices\n",
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149da6a8",
   "metadata": {},
   "source": [
    "In this notebook we will continue to implement different times series forecasting algorithms in order to predict Bitcoin price. In particular, we will build three models:\n",
    "<ul>\n",
    "    <i>Exponential Smoothing</i> - a time series forecasting method for univariate data, which can be used as an alternative for ARIMA family of methods<br>\n",
    "    <i>Prophet</i> - a time-series forecasting model developped by Facebook<br>\n",
    "    <i>PyCaret</i> - a Python version of the Caret machine learning package in R<br>\n",
    "</ul>\n",
    "As always, we will train the models on the training set and evaluate them on the test set. The train and test sets will be the same that we used in the previous notebook.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a8a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from fbprophet import Prophet\n",
    "from sklearn.metrics import r2_score, mean_absolute_percentage_error, mean_squared_error\n",
    "from joblib import Parallel\n",
    "from joblib import delayed\n",
    "from multiprocessing import cpu_count\n",
    "from math import sqrt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#setting figure's default size\n",
    "sns.set(rc={'figure.figsize':(12,5)})\n",
    "plt.rcParams['figure.figsize'] = (12,5)\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "pd.options.display.float_format = \"{:.3f}\".format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bccf40e",
   "metadata": {},
   "source": [
    "First, let's import the data we saved in the previous step. We will use it for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c356e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../PTDD/df.pkl')\n",
    "train = np.load('../PTDD/train_norm.npy')\n",
    "test = np.load('../PTDD/test_norm.npy')\n",
    "btc = pd.read_pickle('../EDA/btc.pkl')\n",
    "btc_price = btc[['Date', 'Close']].sort_values(by='Date', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dd26bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train set observations: {len(train)}')\n",
    "print(f'Test set observations: {len(test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f10343a",
   "metadata": {},
   "source": [
    "And take a look once again at BTC price time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b067c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=btc, x='Date', y='Close', color='Gold')\n",
    "plt.title(f'August 2015 - January 2021 BTC price')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1f5293",
   "metadata": {},
   "source": [
    "### Algorithm 1 - Exponential Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40351ded",
   "metadata": {},
   "source": [
    "Exponential Smoothing (ES) is a time series forecasting algorithm which can be seen as an analog for Box-Jenkins ARIMA family of methonds. Both of them make a prediction as a weighted sum of previous observations. The main difference is that Exponential Smoothing uses an exponentially decreasing weight for previous observations, using parameter named \"alpha\". The bigger alpha is, the less important the older observations become for the prediction. If alpha is small, on the other hand, then the model pays more attention to the older obserbations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cd7a0b",
   "metadata": {},
   "source": [
    "There are three types of Exponential Smoothing:<br>\n",
    "<ol>\n",
    "    1. Single Exponential Smoothing - does not take into account trend and seasonalirt<br>\n",
    "    2. Double Exponential Smoothing - takes trend into account<br>\n",
    "    3. Triple Exponential Smoothing - takes into account both trend and seasonality<br>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4fd056",
   "metadata": {},
   "source": [
    "We know from the previous step that our time series is not stationary. We are not sure if it has clear seasonality, so we will build Double ES model first and than compare it to Triple ES, to see how seasonality will affect the model's performance.<br>\n",
    "It is also important to understand what kind of trend our time series has - additive or multiplicative (we will need to specify it as a hyperparameter). On the graph we see that even though in the last couple month there is an exponential growth, for 5 years before that the general trend seems to be linear, so we will assume the additive trend as our baseline option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ceaf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "train_size = int(len(btc_price) * 0.985)\n",
    "train, test = btc_price[0:train_size], btc_price[train_size:len(btc_price)]\n",
    "print(f'Total Observations: {(len(btc_price))}')\n",
    "print(f'Training Observations: {(len(train))}')\n",
    "print(f'Testing Observations: {(len(test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7eb9609",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a model, fitting on the train set and predicting for the next 30 observations (equivalent to the test set length)\n",
    "des_model = ExponentialSmoothing(train, trend='add', damped=False)\n",
    "des_model_fit = des_model.fit()\n",
    "des_model_pred = des_model_fit.forecast(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee9b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = [x for x in des_model_pred] # List with predicted test values\n",
    "observations_test = [x for x in test['Close']] # List with expected test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb36f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(observations_test, predictions_test)\n",
    "print(f'Test set r-squared for Double ES (no tuning) is: {round(r2, 2)}')\n",
    "mepe = mean_absolute_percentage_error(observations_test, predictions_test)\n",
    "print(f'Test set MAPE for Double ES (no tuning) is: {round(mepe, 2) * 100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc9e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(observations_test, label='Expected')\n",
    "plt.plot(predictions_test, label='Predicted', color='red', alpha=0.5)\n",
    "legend = plt.legend(loc='upper center', shadow=True, fontsize='x-large')\n",
    "plt.title(\"Train set Predicted vs Expected\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad64df5",
   "metadata": {},
   "source": [
    "As we can see, the out-of-box Exponential Smoothing model didn't work pretty much at all. We have a negative r-squared which means the model fits the data even worse than just a straight line. The Mean Absolute Percentage Error is also very high (28% against 2% of ARIMA(2,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c3e37e",
   "metadata": {},
   "source": [
    "In order to make this model work better, we will perform hyperparemeters tuning via GridSearch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d678f7",
   "metadata": {},
   "source": [
    "### Tuning Exponential Smoothing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84859985",
   "metadata": {},
   "source": [
    "Unfortunately, there is no analog of Scikit-Learn GridSearchCV() function for time series. This means we will have to write a few user-defined functions in order to implement hyperparameter tuning process. We will use Walk Forward Validation approach which allows for a model to be updated every time step it recieves new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45301410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ES_forecast(history, config):\n",
    "#     '''This function performs one-ster Exponential Smoothing forecast.\n",
    "#        t - trend, d - dampening, s - seasonality, p - period,\n",
    "#        b - Box-Cox transform (bool), r - remove bias (bool)'''\n",
    "#     t,d,s,p,b,r = config\n",
    "#     history = np.array(history)\n",
    "#     model = ExponentialSmoothing(history, trend=t, dampen=d, seasonal=s, seasonal_periods=p)\n",
    "#     model_fit = model.fit(optimized=True, use_boxcox=b, remove_bias=r)\n",
    "#     y_pred = model_fit.predict(len(history), len(history))\n",
    "#     return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab370c64",
   "metadata": {},
   "source": [
    "Then we will write a function that splits the time series into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f0d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_test_split(ts, test_size):\n",
    "#     '''This function splits the given time series into train and test sets'''\n",
    "#     return ts[:-test_size], ts[-test_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e95c5",
   "metadata": {},
   "source": [
    "Then we will need to measure the errors for our model. We will use both r-squared and Mean Absolute Percentage Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473a4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def r2(expected, predicted):\n",
    "#     '''This function returns r-squared'''\n",
    "#     r2 = r2_score(expected, predicted)\n",
    "#     return round(r2, 2)\n",
    "# def mape(expected, predicted):\n",
    "#     '''This function returns MAPE'''\n",
    "#     mape = mean_absolute_percentage_error(expected, predicted)\n",
    "#     return round(mape, 2) * 100+'%'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64051b",
   "metadata": {},
   "source": [
    "Then we will need a function that will perform one-step validation and return the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a6d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def walk_forward_validaion(ts, test_size, config):\n",
    "#     '''This function:\n",
    "#         1. Splits the time series into train and test sets\n",
    "#         2. Enumerated the number of train set observations\n",
    "#         3. Fits a model on each train set observation\n",
    "#         4. Adds predicted value to predictions list\n",
    "#         5. Adds expected values to history list\n",
    "#         6. Calculates r-squared and MAPE for expected vs. predicted values'''\n",
    "#     prdeictions = []\n",
    "#     train, test = train_test_split(ts, test_size)\n",
    "#     history = [x for x in train]\n",
    "#     for i in range(len(test)):\n",
    "#         y_pred = ES_forecast(history, config)\n",
    "#         predictions.append(y_pred)\n",
    "#         history.append(test[i])\n",
    "#     r2 = r2(test, predictions)\n",
    "# #     print(f'r-squared for Exponential Smoothing (config: {config}) is: {r2}')\n",
    "#     mape = mape(test, predictions)\n",
    "# #     print(f'Mean Absolute Percentage Error for Exponential Smoothing (config: {config}) is: {mape}')\n",
    "#     return (r2, mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0342b5",
   "metadata": {},
   "source": [
    "Now we will write one more function. In case there is an error it will retrun None. If everything runs smoothly it will return the results of model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3582de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_model(ts, test_size, config, defug=False):\n",
    "#     '''This function performs walk forward validation'''\n",
    "#     result=None\n",
    "#     conf = str(config)\n",
    "#     if debug:\n",
    "#         result = walk_forward_validation(ts, test_size, config)\n",
    "#     else:\n",
    "#         error = None\n",
    "#     if result is not None:\n",
    "#         print(print(' > Model[%s] %.3f' % (conf, result)))\n",
    "#     return(conf, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a580a",
   "metadata": {},
   "source": [
    "Grid search is a brute force method, so it requires a lot of computational power and time. We can reduce this time by parallelizing the grid search and making use of all of your machine's CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba9fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def grid_search_NO(ts, config_list, test_size, parallel=True):\n",
    "#     '''This function performs grid search over different mode parameters.\n",
    "#         By default it uses parallel computations.\n",
    "#         It returns results of the different configurations evaluation.'''\n",
    "#     scores = None\n",
    "#     if parallel:\n",
    "#         parallelizer = Parallel(n_jobs=cpu_count(), backend='multiprocessing')\n",
    "#         tasks = (delayed(eval_model)(ts, test_size, config) for config in config_list)\n",
    "#         scores = parallelizer(tasks)\n",
    "#     else:\n",
    "#         scores = [eval_model(ts, test_size, config) for config in config_list]\n",
    "#     # removing empty results\n",
    "#     scores = [res for res in scores if res[1] != None]\n",
    "#     scores.sort(key=lambda tup:tup[1])\n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9a618f",
   "metadata": {},
   "source": [
    "Now we will need to create a list of model parameters configurations which we will use in our grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db972dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ES_configs(seasonal=[None]):\n",
    "#     '''This function returns a list of different model configurations\n",
    "#         which are used in grid search'''\n",
    "#     configs = []\n",
    "#     # trend parameters\n",
    "#     t_params = ['add', 'mul', None]\n",
    "#     # dampening parameters\n",
    "#     d_params = [True, False]\n",
    "#     # seasonality parameters)\n",
    "#     s_params = ['add', 'mul', None]\n",
    "#     # period parameters (default=None)\n",
    "#     p_params = seasonal\n",
    "#     # Box-Cox parameters\n",
    "#     b_params = [True, False]\n",
    "#     # remove bias parameters\n",
    "#     r_params = [True, False]\n",
    "#     for t in t_params:\n",
    "#         for d in d_params:\n",
    "#             for s in s_params:\n",
    "#                 for p in p_params:\n",
    "#                     for b in b_params:\n",
    "#                         for r in r_params:\n",
    "#                             config = [t,d,s,p,b,r]\n",
    "#                             configs.append(config)\n",
    "#     return configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312e04cc",
   "metadata": {},
   "source": [
    "Finally, we can implement the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95b37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_price_for_func = btc_price.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9d1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search ets models for monthly car sales\n",
    "from math import sqrt\n",
    "from multiprocessing import cpu_count\n",
    "from joblib import Parallel\n",
    "from joblib import delayed\n",
    "from warnings import catch_warnings\n",
    "from warnings import filterwarnings\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pandas import read_csv\n",
    "from numpy import array\n",
    "\n",
    "# one-step Holt Winter’s Exponential Smoothing forecast\n",
    "def exp_smoothing_forecast(history, config):\n",
    "\tt,d,s,p,b,r = config\n",
    "\t# define model\n",
    "\thistory = array(history)\n",
    "\tmodel = ExponentialSmoothing(history, trend=t, damped=d, seasonal=s, seasonal_periods=p)\n",
    "\t# fit model\n",
    "\tmodel_fit = model.fit(optimized=True, use_boxcox=b, remove_bias=r)\n",
    "\t# make one step forecast\n",
    "\tyhat = model_fit.predict(len(history), len(history))\n",
    "\treturn yhat[0]\n",
    "\n",
    "# root mean squared error or rmse\n",
    "def measure_rmse(actual, predicted):\n",
    "\treturn sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "\treturn data[:-n_test], data[-n_test:]\n",
    "\n",
    "# walk-forward validation for univariate data\n",
    "def walk_forward_validation(data, n_test, cfg):\n",
    "\tpredictions = list()\n",
    "\t# split dataset\n",
    "\ttrain, test = train_test_split(data, n_test)\n",
    "\t# seed history with training dataset\n",
    "\thistory = [x for x in train]\n",
    "\t# step over each time-step in the test set\n",
    "\tfor i in range(len(test)):\n",
    "\t\t# fit model and make forecast for history\n",
    "\t\tyhat = exp_smoothing_forecast(history, cfg)\n",
    "\t\t# store forecast in list of predictions\n",
    "\t\tpredictions.append(yhat)\n",
    "\t\t# add actual observation to history for the next loop\n",
    "\t\thistory.append(test[i])\n",
    "\t# estimate prediction error\n",
    "\terror = measure_rmse(test, predictions)\n",
    "\treturn error\n",
    "\n",
    "# score a model, return None on failure\n",
    "def score_model(data, n_test, cfg, debug=False):\n",
    "\tresult = None\n",
    "\t# convert config to a key\n",
    "\tkey = str(cfg)\n",
    "\t# show all warnings and fail on exception if debugging\n",
    "\tif debug:\n",
    "\t\tresult = walk_forward_validation(data, n_test, cfg)\n",
    "\telse:\n",
    "\t\t# one failure during model validation suggests an unstable config\n",
    "\t\ttry:\n",
    "\t\t\t# never show warnings when grid searching, too noisy\n",
    "\t\t\twith catch_warnings():\n",
    "\t\t\t\tfilterwarnings(\"ignore\")\n",
    "\t\t\t\tresult = walk_forward_validation(data, n_test, cfg)\n",
    "\t\texcept:\n",
    "\t\t\terror = None\n",
    "\t# check for an interesting result\n",
    "\tif result is not None:\n",
    "\t\tprint(' > Model[%s] %.3f' % (key, result))\n",
    "\treturn (key, result)\n",
    "\n",
    "# grid search configs\n",
    "def grid_search(data, cfg_list, n_test, parallel=True):\n",
    "\tscores = None\n",
    "\tif parallel:\n",
    "\t\t# execute configs in parallel\n",
    "\t\texecutor = Parallel(n_jobs=cpu_count(), backend='multiprocessing')\n",
    "\t\ttasks = (delayed(score_model)(data, n_test, cfg) for cfg in cfg_list)\n",
    "\t\tscores = executor(tasks)\n",
    "\telse:\n",
    "\t\tscores = [score_model(data, n_test, cfg) for cfg in cfg_list]\n",
    "\t# remove empty results\n",
    "\tscores = [r for r in scores if r[1] != None]\n",
    "\t# sort configs by error, asc\n",
    "\tscores.sort(key=lambda tup: tup[1])\n",
    "\treturn scores\n",
    "\n",
    "# create a set of exponential smoothing configs to try\n",
    "def exp_smoothing_configs(seasonal=[None]):\n",
    "\tmodels = list()\n",
    "\t# define config lists\n",
    "\tt_params = ['add', 'mul', None]\n",
    "\td_params = [True, False]\n",
    "\ts_params = ['add', 'mul', None]\n",
    "\tp_params = seasonal\n",
    "\tb_params = [True, False]\n",
    "\tr_params = [True, False]\n",
    "\t# create config instances\n",
    "\tfor t in t_params:\n",
    "\t\tfor d in d_params:\n",
    "\t\t\tfor s in s_params:\n",
    "\t\t\t\tfor p in p_params:\n",
    "\t\t\t\t\tfor b in b_params:\n",
    "\t\t\t\t\t\tfor r in r_params:\n",
    "\t\t\t\t\t\t\tcfg = [t,d,s,p,b,r]\n",
    "\t\t\t\t\t\t\tmodels.append(cfg)\n",
    "\treturn models\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t# load dataset\n",
    "\tseries = read_csv('monthly-car-sales.csv', header=0, index_col=0)\n",
    "\tdata = series.values\n",
    "\t# data split\n",
    "\tn_test = 12\n",
    "\t# model configs\n",
    "\tcfg_list = exp_smoothing_configs(seasonal=[0,6,12])\n",
    "\t# grid search\n",
    "\tscores = grid_search(data[:,0], cfg_list, n_test)\n",
    "\tprint('done')\n",
    "\t# list top 3 configs\n",
    "\tfor cfg, error in scores[:3]:\n",
    "\t\tprint(cfg, error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
